from typing import (
    Any,
    Callable,
    Dict,
    Hashable,
    List,
    Literal,
    Sequence,
    Tuple,
    Union,
)

import numpy as np
from dask.array.core import Array
from dask.base import tokenize
from dask.core import flatten
from dask.highlevelgraph import HighLevelGraph
from dask.utils import apply
from xarray import DataArray

from xarray_multiscale.reducers import WindowedReducer
from xarray_multiscale.util import broadcast_to_rank, adjust_shape, logn
from xarray_multiscale.chunks import normalize_chunks, align_chunks
import numpy.typing as npt

PadMode = Literal["crop", "pad"]


def multiscale(
    array: npt.NDArray[Any],
    reduction: WindowedReducer,
    scale_factors: Union[Sequence[int], int],
    pad_mode: PadMode = "crop",
    preserve_dtype: bool = True,
    chunks: Union[str, Sequence[int], Dict[Hashable, int]] = "auto",
    chunk_merge_only: bool = False,
    chained: bool = True,
) -> List[DataArray]:
    """
    Generate a coordinate-aware multiscale representation of an array.

    Parameters
    ----------
    array : Array-like, e.g. Numpy array, Dask array
        The array to be downscaled.

    reduction : callable
        A function that aggregates chunks of data over windows.
        See `` for the expected
        signature of this callable.

    scale_factors : int or sequence of ints
        The desired downscaling factors, one for each axis.
        If a single int is provide, it will be broadcasted to all axes.

    pad_mode : string or None, default=None
        How arrays should be padded prior to downscaling in order to ensure
        that each array dimension is evenly divisible by the respective
        scale factor. When set to `None` (default), the input will be
        cropped before downscaling if its dimensions are not divisible by
        ``scale_factors``.

    preserve_dtype : bool, default=True
        Determines whether the multiresolution arrays are all explicitly
        cast to the same dtype as the input.

    chunks : sequence or dict of ints, or the string "auto" (default)
        Set the chunk layout of the output arrays.
        If `chunks` is set to "auto" (the default), then chunk sizes will
        decrease with each level of downsscaling. Otherwise,
        this keyword argument will be passed to the `xarray.DataArray.chunk`
        method for each output array, producing a list of arrays with the same
        chunk size. Note that rechunking can be computationally expensive
        for arrays with many chunks.

    chunk_merge_only : bool, default=False
        If False (default) and `chunks` is not set to "auto",
        then all output arrays are rechunked regardless of their chunk size.

        If True and `chunks` is not set to "auto", then output arrays will only
        be rechunked if their chunk size is smaller in any dimension
        than the value of `chunks`.
        This avoid potentially unecessary rechunking operations
        from being applied to output arrays with chunks larger
        than `chunks`.

    chained : bool, default=True
        If True (default), the nth downscaled array is generated by
        applying the reduction function on the n-1th downscaled array with
        the user-supplied `scale_factors`. This means that the nth
        downscaled array directly depends on the n-1th downscaled array.
        Note that nonlinear reductions like the windowed mode may give
        inaccurate results with `chained` set to True.

        If False, the nth downscaled array is generated by applying the
        reduction function on the 0th downscaled array
        (i.e., the input array) with the `scale_factors` raised to the nth
        power. This means that the nth downscaled array directly depends
        on the input array.

    Returns
    -------
    result : list of DataArrays
        The `coords` attribute of these DataArrays track the changing
        offset (if any) induced by the downsampling operation.
        Additionally, the scale factors are stored each DataArray's
        attrs propery under the key `scale_factors`


    Examples
    --------
    >>> import numpy as np
    >>> from xarray_multiscale import multiscale
    >>> from xarray_multiscale.reducers import windowed_mean
    >>> multiscale(np.arange(4), windowed_mean, 2)
    [<xarray.DataArray (dim_0: 4)>
    array([0, 1, 2, 3])
    Coordinates:
      * dim_0    (dim_0) float64 0.0 1.0 2.0 3.0, <xarray.DataArray (dim_0: 2)>
    array([0, 2])
    Coordinates:
      * dim_0    (dim_0) float64 0.5 2.5, <xarray.DataArray (dim_0: 1)>
    array([1])
    Coordinates:
      * dim_0    (dim_0) float64 1.5]
    """
    scale_factors = broadcast_to_rank(scale_factors, array.ndim)
    normalized_array = normalize_array(array, scale_factors, pad_mode=None)
    do_padding = not (pad_mode == "crop")
    levels = range(
        get_downscale_depth(normalized_array.shape, scale_factors, pad=do_padding)
    )
    scales = tuple(tuple(s**level for s in scale_factors) for level in levels)
    result = []
    for level in levels:
        if level == 0:
            result.append(normalized_array)
        if chained:
            scale = scale_factors
            source = result[-1]
        else:
            scale = scales[level]
            source = result[0]
        result.append(downscale(source, reduction, scale, pad_mode=pad_mode))

    if preserve_dtype:
        result = [r.astype(array.dtype) for r in result]

    if normalized_array.chunks is not None:
        new_chunks = [normalize_chunks(r, chunks, chunk_merge_only) for r in result]
        result = [r.chunk(ch) for r, ch in zip(result, new_chunks)]

    return result


def normalize_array(
    array: Any, scale_factors: Sequence[int], pad_mode: Union[str, None]
) -> DataArray:
    """
    Ingest an array in preparation for downscaling by converting to DataArray
    and cropping / padding as needed.
    """
    if isinstance(array, DataArray):
        # if the input is a xarray.DataArray
        # assign a new variable to the DataArray and use
        # `array` to refer to the data property of the input
        data = array.data
        dims = array.dims
        # ensure that key order matches dimension order
        coords = {d: array.coords[d] for d in dims}
        attrs = array.attrs
        name = array.name
    else:
        data = array
        dims = tuple(f"dim_{d}" for d in range(data.ndim))
        offset = 0.0
        coords = {
            dim: DataArray(offset + np.arange(shp, dtype="float"), dims=dim)
            for dim, shp in zip(dims, array.shape)
        }
        name = None
        attrs = {}

    dataArray = DataArray(data=data, coords=coords, dims=dims, attrs=attrs, name=name)
    reshaped = adjust_shape(dataArray, scale_factors=scale_factors, mode=pad_mode)
    return reshaped


def downscale_dask(
    array: Any,
    reduction: WindowedReducer,
    scale_factors: Union[int, Sequence[int], Dict[int, int]],
    **kwargs: Any,
) -> Any:

    if not np.all((np.array(array.shape) % np.array(scale_factors)) == 0):
        raise ValueError(
            f"Coarsening factors {scale_factors} do not align with array shape {array.shape}."
        )

    array = align_chunks(array, scale_factors)
    name: str = "downscale-" + tokenize(reduction, array, scale_factors)
    dsk = {
        (name,) + key[1:]: (apply, reduction, [key, scale_factors], kwargs)
        for key in flatten(array.__dask_keys__())
    }
    chunks = tuple(
        tuple(int(size // scale_factors[axis]) for size in sizes)
        for axis, sizes in enumerate(array.chunks)
    )

    meta = reduction(
        np.empty(scale_factors, dtype=array.dtype), scale_factors, **kwargs
    )
    graph = HighLevelGraph.from_collections(name, dsk, dependencies=[array])
    return Array(graph, name, chunks, meta=meta)


def downscale(
    array: DataArray,
    reduction: WindowedReducer,
    scale_factors: Sequence[int],
    pad_mode: str,
    **kwargs: Any,
) -> Any:

    to_downscale = normalize_array(array, scale_factors, pad_mode=pad_mode)
    if to_downscale.chunks is not None:
        downscaled_data = downscale_dask(
            to_downscale.data, reduction, scale_factors, **kwargs
        )
    else:
        downscaled_data = reduction(to_downscale.data, scale_factors)

    downscaled_coords = downscale_coords(to_downscale, scale_factors)
    return DataArray(downscaled_data, downscaled_coords, attrs=array.attrs)


def downscale_coords(
    array: DataArray, scale_factors: Sequence[int]
) -> Dict[Hashable, Any]:
    """
    Downscale coordinates by taking the windowed mean of each coordinate array.
    """
    new_coords = {}
    for (
        coord_name,
        coord,
    ) in array.coords.items():
        coarsening_dims = {
            d: scale_factors[idx] for idx, d in enumerate(array.dims) if d in coord.dims
        }
        new_coords[coord_name] = coord.coarsen(coarsening_dims).mean()
    return new_coords


def get_downscale_depth(
    shape: Tuple[int, ...], scale_factors: Sequence[int], pad: bool = False
) -> int:
    """
    For a shape and a sequence of scale factors, calculate the
    maximum possible number of downscaling operations.
    If any element of `scale_factors` is greater than the
    corresponding shape, this function returns 0.
    If all `scale factors` are 1, this function returns 0.
    """
    if len(shape) != len(scale_factors):
        raise ValueError(
            f"Shape (length == {len(shape)} ) and scale factors (length == {len(scale_factors)}) do not align."
        )

    _scale_factors = np.array(scale_factors).astype("int")
    _shape = np.array(shape).astype("int")
    valid = _scale_factors > 1
    if not valid.any():
        result = 0
    else:
        if pad:
            depths = np.ceil(logn(_shape[valid], _scale_factors[valid])).astype("int")
        else:
            depths = np.floor(logn(_shape[valid], _scale_factors[valid])).astype("int")
        result = min(depths)
    return result
